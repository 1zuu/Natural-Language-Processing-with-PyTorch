{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a72305-bd93-460d-8315-9b74f189f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<SOS>'.lower()\n",
    "END_TOKEN = '<EOS>'.lower()\n",
    "\n",
    "DATA_PATH = 'data/poetry.txt'\n",
    "\n",
    "CUTOFF = 0.8\n",
    "EPOCHES = 2000\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 128\n",
    "LATENT_DIM = 25\n",
    "EMBEDDING_DIM = 50\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2f401f-6b85-433b-a47e-762a8704ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchnlp.encoders.text import *\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66487d38-0c8d-4bcf-bb7b-dc6e9588aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    input_lines = []\n",
    "    target_lines = []\n",
    "    for line in open(DATA_PATH):\n",
    "        line = line.strip()\n",
    "        \n",
    "        input_line = '{} {}'.format(START_TOKEN, line)\n",
    "        target_line = '{} {}'.format(line, END_TOKEN)\n",
    "        \n",
    "        input_lines.append(input_line)\n",
    "        target_lines.append(target_line)\n",
    "        \n",
    "    return input_lines, target_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6704edc9-b7ce-4052-9a46-d4de64504ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_line(line, encoder):\n",
    "    return encoder.encode(line)\n",
    "\n",
    "def tokenize_lines(lines, encoder):\n",
    "    return [tokenize_line(line, encoder) for line in lines]\n",
    "\n",
    "def extract_max_length(all_seq_lengths):\n",
    "    return torch.max(all_seq_lengths).item()\n",
    "    \n",
    "def padding_line(tokens, MAX_LENGTH):\n",
    "    return pad_tensor(\n",
    "                    tokens, \n",
    "                    length = MAX_LENGTH\n",
    "                    )\n",
    "\n",
    "def padding_lines(all_tokens):\n",
    "    return stack_and_pad_tensors(\n",
    "                    all_tokens\n",
    "                    )\n",
    "\n",
    "def process_line(line, encoder, MAX_LENGTH):\n",
    "    tokens = tokenize_line(line, encoder)\n",
    "    padded = padding_line(tokens, MAX_LENGTH)\n",
    "    return padded\n",
    "\n",
    "def process_lines(lines, encoder):\n",
    "    all_tokens = tokenize_lines(lines, encoder)\n",
    "    all_padded = padding_lines(all_tokens)\n",
    "    return all_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faa2a602-30f0-4084-ad12-6dfc53ad26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "*** Make target Sequence OneHot ***\n",
    "\n",
    "    The Reason is that Sparse Categorical Cross Entropy won't work when Target is a Sequence.\n",
    "'''\n",
    "\n",
    "def make_one_hot_target_sequences(MAX_LENGTH, vocabulary, target_sequences):\n",
    "    onehot_target_sequences = torch.empty(\n",
    "                                    len(target_sequences),\n",
    "                                    MAX_LENGTH,\n",
    "                                    len(vocabulary),\n",
    "                                    dtype=torch.float32\n",
    "                                        )\n",
    "    \n",
    "    for idx, target_sequence in enumerate(target_sequences):\n",
    "        onehot_target_sequence = torch.nn.functional.one_hot(\n",
    "                                                    target_sequence, \n",
    "                                                    num_classes = len(vocabulary)\n",
    "                                                    )\n",
    "        onehot_target_sequences[idx, :, :] = onehot_target_sequence\n",
    "        \n",
    "    return onehot_target_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44788aa-e828-4e00-918b-38394a300beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    input_lines, target_lines = load_data()\n",
    "    all_lines = input_lines + target_lines\n",
    "    encoder = StaticTokenizerEncoder(\n",
    "                                all_lines, \n",
    "                                tokenize = lambda s: s.split()\n",
    "                                )\n",
    "\n",
    "    input_sequences, input_seq_lengths = process_lines(input_lines, encoder)\n",
    "    target_sequences, target_seq_lengths = process_lines(target_lines, encoder)\n",
    "    all_sequences, all_seq_lengths = process_lines(all_lines, encoder)\n",
    "\n",
    "    MAX_LENGTH = extract_max_length(all_seq_lengths)\n",
    "    vocabulary = encoder.vocab\n",
    "    \n",
    "    assert START_TOKEN in vocabulary, 'START_TOKEN NOT FOUND' \n",
    "    assert END_TOKEN in vocabulary, 'END_TOKEN NOT FOUND' \n",
    "    \n",
    "    target_sequences = make_one_hot_target_sequences(\n",
    "                                                    MAX_LENGTH,\n",
    "                                                    vocabulary, \n",
    "                                                    target_sequences\n",
    "                                                    )\n",
    "    \n",
    "    return input_sequences, input_seq_lengths, target_sequences, target_seq_lengths, encoder, MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4f1b79e-549a-439b-8393-d4cd6775ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageGenerator(torch.nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                VOCAB_SIZE,\n",
    "                DEVICE\n",
    "                ):\n",
    "        super(LanguageGenerator,self).__init__()\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "        self.lstm_layer = torch.nn.LSTM(\n",
    "                                    input_size=EMBEDDING_DIM, \n",
    "                                    hidden_size=LATENT_DIM,\n",
    "                                    num_layers=NUM_LAYERS, \n",
    "                                    batch_first=True\n",
    "                                    ) \n",
    "\n",
    "        self.linear_layer = torch.nn.Linear(LATENT_DIM, VOCAB_SIZE)\n",
    "        \n",
    "        \n",
    "        self.DEVICE = DEVICE\n",
    "        self.VOCAB_SIZE = VOCAB_SIZE\n",
    "\n",
    "    def forward(self, x, memory_init, CHUNK_SIZE):\n",
    "        x = x.long()\n",
    "        \n",
    "        x = self.embedding_layer(x)\n",
    "        x, _ = self.lstm_layer(x, memory_init) \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        output shape : (CHUNK_SIZE, MAX_LENGTH, LATENT_DIM)\n",
    "             --> Always return_sequences = True\n",
    "        \n",
    "        '''\n",
    "        x = self.linear_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb3b935-558f-450a-a46b-6054dc72660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST MODEL\n",
    "# device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda:0')\n",
    "# model = LanguageGenerator(3921, device).to(device)\n",
    "# sample_seq = torch.tensor([[1,2,3,4,5,6]]).to(device)\n",
    "# sample_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded47a63-1a93-4e2f-894f-2ed757a60988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelingPyTorch(object):\n",
    "    def __init__(self):\n",
    "        input_sequences, input_seq_lengths, target_sequences, target_seq_lengths, encoder, MAX_LENGTH = process_data()\n",
    "        \n",
    "        dataset = TensorDataset(\n",
    "                        input_sequences, \n",
    "                        target_sequences\n",
    "                                )\n",
    "        \n",
    "        train_data, valid_data = torch.utils.data.random_split(dataset, \n",
    "                                                              [int(len(input_sequences) * CUTOFF), len(input_sequences) - int(len(input_sequences) * CUTOFF)])\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "                            train_data, \n",
    "                            shuffle=False, \n",
    "                            drop_last=True,\n",
    "                            batch_size=BATCH_SIZE\n",
    "                                )\n",
    "    \n",
    "        valid_loader = DataLoader(\n",
    "                            valid_data, \n",
    "                            shuffle=False, \n",
    "                            drop_last=True,\n",
    "                            batch_size=BATCH_SIZE\n",
    "                                )\n",
    "        \n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.VOCAB_SIZE = len(encoder.vocab)\n",
    "        self.DEVICE = torch.device('cpu' if not torch.cuda.is_available() else 'cuda:0') \n",
    "        \n",
    "        h_init = torch.zeros(\n",
    "                        NUM_LAYERS, \n",
    "                        CHUNK_SIZE, \n",
    "                        LATENT_DIM\n",
    "                        ).to(self.DEVICE)\n",
    "        \n",
    "        c_init = torch.zeros(\n",
    "                        NUM_LAYERS, \n",
    "                        CHUNK_SIZE, \n",
    "                        LATENT_DIM\n",
    "                        ).to(self.DEVICE)\n",
    "        \n",
    "        self.memory_init = (h_init, c_init)\n",
    "            \n",
    "    def language_generator(self):        \n",
    "        model = LanguageGenerator(\n",
    "                                    self.VOCAB_SIZE, \n",
    "                                    self.DEVICE\n",
    "                                     )\n",
    "        \n",
    "        self.model = model.to(self.DEVICE)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(\n",
    "                                    self.model.parameters(),\n",
    "                                    lr = LEARNING_RATE\n",
    "                                        )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        \n",
    "        train_loss_epoch = 0\n",
    "        valid_loss_epoch = 0\n",
    "        self.model.train()\n",
    "        \n",
    "        for Xbatch, Ybatch in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            Xbatch = Xbatch.to(self.DEVICE)\n",
    "            Ybatch = Ybatch.to(self.DEVICE)\n",
    "        \n",
    "            CHUNK_SIZE = Xbatch.shape[0]\n",
    "            \n",
    "            Pbatch = self.model(Xbatch, self.memory_init, CHUNK_SIZE).to(self.DEVICE)\n",
    "\n",
    "            train_loss = self.criterion(Pbatch, Ybatch)\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            train_loss_epoch += train_loss.item()\n",
    "            \n",
    "        train_loss_epoch = train_loss_epoch / len(self.train_loader)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        for Xbatch, Ybatch in self.valid_loader:\n",
    "\n",
    "            Xbatch = Xbatch.to(self.DEVICE)\n",
    "            Ybatch = Ybatch.to(self.DEVICE)\n",
    "        \n",
    "            CHUNK_SIZE = Xbatch.shape[0]\n",
    "            \n",
    "            Pbatch = self.model(Xbatch,self.memory_init, CHUNK_SIZE).to(self.DEVICE)\n",
    "\n",
    "            valid_loss = self.criterion(Pbatch, Ybatch)\n",
    "            \n",
    "            valid_loss_epoch += valid_loss.item()\n",
    "            \n",
    "        valid_loss_epoch = valid_loss_epoch / len(self.train_loader)\n",
    "        \n",
    "        return train_loss_epoch, valid_loss_epoch\n",
    "            \n",
    "    def train_loop(self):\n",
    "        train_loss, valid_loss = [], []\n",
    "        for epoch in range(EPOCHES):\n",
    "            train_loss_epoch, valid_loss_epoch = self.train_epoch()\n",
    "            \n",
    "            train_loss.append(train_loss_epoch)\n",
    "            valid_loss.append(valid_loss_epoch)\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print('EPOCH : {}, TRAIN LOSS : {}, VALID LOSS : {}'.format(epoch+1, train_loss_epoch, valid_loss_epoch))\n",
    "        \n",
    "        self.train_loss = train_loss\n",
    "        self.valid_loss = valid_loss\n",
    "        \n",
    "    def plot_cross_entropy(self):\n",
    "        plt.plot(self.train_loss, label='loss')\n",
    "        plt.plot(self.valid_loss, label='val_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self):\n",
    "        self.language_generator()\n",
    "        self.train_loop()\n",
    "        self.plot_cross_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d4e26f-c851-458e-b254-8250637fedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMP = LanguageModelingPyTorch()\n",
    "LMP.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798fe15-86c6-492a-8fc6-bd6efb4def75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
